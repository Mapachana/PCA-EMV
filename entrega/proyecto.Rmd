---
title: "Practica PCA"
author: Ana Buendia Ruiz-Azuaga
output:
  html_document: default
  pdf_document: default
---

```{r,include=FALSE}
# install.packages("EnvStats")
library(EnvStats)
library(car)
library(foreign)

```

***

\begin{center}
 \tableofcontents
 \textsc{--}\\
\end{center}



***
\section{Información sobre el Dataset}


El fichero de datos DB_3.sav contiene las variables ZTLIBROP, ZTEJERCI, ZTPOBACT, ZTENERGI, ZPSERVI, ZPAGRICU, ZTMEDICO, ZESPVIDA, ZTMINFAN y ZPOBDENS que respectivamente son los valores para cada país del mundo de:

## Variables

• Número de libros publicados (ZTLIBROP).

• Cociente entre número de individuos en ejército de tierra y población total del estado (ZTEJERCI).

• Cociente entre población activa y total (ZTPOBACT).

• Tasa de consumo energético (ZTENERGI).

• Población del sector servicios (ZPSERVI).

• Población del sector agrícola (ZPAGRICU).

• Tasa de médicos por habitante (ZTMEDICO).

• Esperanza de vida (ZESPVIDA).

• Tasa de mortalidad infantil (ZTMINFAN).

• Densidad de población (ZPOBDENS).

• Porcentaje de la población urbana (POBURB).


\subsection{Otra información de interés}
  
El dataset contiene un total de 34 instancias con 12 variables, contando con la etiqueta del país al que corresponde las 11 variables mencionadas anteriormente.

\section{Primer acercamiento}

Cargamos los datos a partir del fichero *DB_3.sav*.
```{r,echo=T,include=F}
getwd()

datos_enteros<-read.spss("DB_3.sav", to.data.frame = TRUE)
#Cogemos las variables de interes, es decir, elimino las etiquetas del pais
datos<-datos_enteros[,2:12]
```

\subsection{Primera visualización}

Echamos un primer vistazo a los datos, observando directamente el dataframe en el que vienen, para ver si los datos efectivamente están estandarizados (datos numéricos, continuos, tanto negativos como positivos y entorno al 0), y para comprobar la existencia de *NA*s.
```{r,echo=T}
head(datos)
```

Vemos tanto la existencia de valores perdidos, y que las variables efectivamente están estandarizadas, ya que variables que intuitivamente tomarían valores enteros y relativamente altos como la esperanza de vida o los libros publicados contienen decimales, valores positivos y negativos y todos se encuentran cercanos a 0.

\subsection{Tratamiento de los *NA*}

Vamos a ver si hay, donde, y cuantos *NA* hay en los datos
```{r,echo=T}
cbind(apply(is.na(datos),2,sum),apply(is.na(datos),2,sum)/dim(datos)[1])
```

Vemos que apenas hay variables con datos faltantes, pero una de las variables (*ZTLIBROP*) tiene uno de sus registros faltantes. Como, al ser un único valor perdido en un conjunto de datos con 34 instancias es claro que tenemos menos del 5% de valores perdidos, podemos imputarlo con el valor de la media (ya que es una variable cuantitativa) sin que afecte significamente al resultado del análisis.

```{r,echo=T}
not_available<-function(data,na.rm=F){
  data[is.na(data)]<-mean(data,na.rm=T)
  data
}

datos_pca<-as.data.frame(apply(datos, 2, not_available))
```

Comprobamos ahora que se ha imputado correctamente el valor perdido:

```{r,echo=T}
cbind(apply(is.na(datos_pca),2,sum),apply(is.na(datos_pca),2,sum)/dim(datos_pca)[1])
```

En efecto se ha realizado la imputación de valores perdidos sin ningún problema.

## Recodificación

En este caso no es necesaria.

# Exploración Univariante

## Exploración descriptiva

En este apartado iremos variable por variable obteniendo los resultados de aplicar diferentes medidas descriptivas, clásicas y resistentes, de centralidad, forma y dispersión.

```{r,echo=T}
#Definimos las medidas resistentes
PMC<-function(x){ return((as.double(quantile(x,0.25))+as.double(quantile(x,0.75)))/2)}

trimedia<-function(x){return((median(x)+PMC(x))/2)}

centrimedia<-function(x){
  indices<-(x>quantile(x,0.25)&x<quantile(x,0.75))
  valores<-x[indices]
  return(sum(valores)/length(valores))
}

RIQ<-function(x){return(quantile(x,0.75)-quantile(x,0.25))}

MEDA<-function(x){return(median(abs(x-median(x))))}

CVc<-function(x){return((quantile(x,0.75)-quantile(x,0.25))/(quantile(x,0.75)+quantile(x,0.25)))}

H1<-function(x){return((quantile(x,0.25)+quantile(x,0.75)-2*median(x))/(2*median(x)))}
H2<-function(x){return(median(x)-(quantile(x,0.1)+quantile(x,0.9))/(2))}
H3<-function(x){return(H2(x)/median(x))}

#Creamos una función que aplique todas estas medidas

descriptivo<-function(x){
  
  temp<-rbind(PMC(x),trimedia(x),centrimedia(x))
  rownames(temp)<-c("PMC","Trimedia","Centrimedia")
  centralidad<-list(clasica=list(media=mean(x)),resistente=temp)
  
  temp<-rbind(RIQ(x),MEDA(x),CVc(x))
  rownames(temp)<-c("Rango Inter-Cuartílico","MEDA","CVc")
  dispersion<-list(clasica=list(desviación_típica=sd(x),Coef_varización=sd(x)/mean(x),rango=range(x)),resistente=temp)
  
  temp<-rbind(H1(x),H2(x),H3(x))
  rownames(temp)<-c("Asimetría de Yule","Asimetría de Kelly","Asimetría de Kelly adimensional")
  forma<-list(clasica=list(skewness=skewness(x),kurtosis=kurtosis(x)),resistente=temp)
  cat(names(x))
  return(list(centralidad=centralidad,dispersion=dispersion,forma=forma))
}
```

Aplicamos la función para cada una de las variables:

\newpage

**ZPOBDENS**
```{r,echo=T}
descriptivo(datos_pca[,1])
hist(col="darkblue",datos_pca[,1],main="ZPOBDENS")
```

Las medidas resistentes de centralidad están ligeramente desplazadas hacia la izquierda. Tenemos un valor del *MEDA* inferior al de la desviación típica. En los estimadores de simetría, obtenemos que existe cierta asimetría, y el valor de *kurtosis* indica una acumulación de los datos.

\newpage
**ZMINFAN**

```{r,echo=T}
descriptivo(datos_pca[,2])
hist(col="darkblue",datos_pca[,2],main="ZMINFAN")
```

Lo primero que llama la atención en el histograma es que a prmera vista parecen dos normales pegadas, pero esto puede deberse a que tenemos muy pocos datos en el conjunto a estudiar y sea una normal con una cola larga.

La media se encuentra ligeramente desviada a la izquierda, y de nuevo la MEDA es menor que la desviación típica.

Vemos en el coeficiente de curtosis que la distribución es muy aplanada y asimetría, lo que se corresponde con el histograma.

\newpage

**ZESPVIDA**  

```{r,echo=T}
descriptivo(datos_pca[,3])
hist(col="darkblue",datos_pca[,3],main="ZESPVIDA")
```

Las medidas de centralidad se encuentran desplazadas a la derecha, y la MEDA vuelve a ser menor que la desviación típica. De nuevo mirando el coeficiente de curtosis vemos una distribución bastante aplanada y con asimetría.

\newpage

**ZPOBURB** 

```{r,echo=T}
descriptivo(datos_pca[,4])
hist(col="darkblue",datos_pca[,4],main="ZPOBURB")
```
Esta distribución parece bastante plana mirando el histograma, ningún valor parece predominar demasiado sobre los otros, pero de nuevo se necesitarían más datos para confirmar.
 
La media y medidas de centralidad están muy cercanas a 0 y la curtosis indica que la distribución es bastante plana.


\newpage

**ZTMEDICO** 

```{r,echo=T}
descriptivo(datos_pca[,5])
hist(col="darkblue",datos_pca[,5],main="ZTMEDICO")
```

Las medidas de centralidad se encuentran desplazadas hacia la izquierda y el CVc tiene un valor muy bajo. La curtosis indica que la distribución es plana

También llama la atención como las dos primeras columnas son muy altas y a partir de ellas el resto del histograma se asemeja más a una normal.


\newpage

**ZPAGRICU**  

```{r,echo=T}
descriptivo(datos_pca[,6])
hist(col="darkblue",datos_pca[,6],main="ZPAGRICU")
```

En este caso el histograma se asemeja más al de una normal que en las anteriores variables estudiadas, pese a que se aprecia una asimetría y distribución bastante plana.

Las medidas de centralidad están desviadas ligeramente a la izquierda y la curtosis indica que la distribución es bastante plana.

\newpage

**ZPSERVI** 

```{r,echo=T}
descriptivo(datos_pca[,7])
hist(col="darkblue",datos_pca[,7],main="ZPSERVI")
```

Observamos que las medidas de centralidad están desviadas a la derecha, el MEDA es menor que la desviación típica y la curtosis indica que la distribución es plana.

Además, es claro viendo el histograma que la distribución no se asemeja demasiado a la normal, y es posible que la última columna, al tener frecuencia 1, se trae de un outlier.

\newpage

**ZTLIBROP** 

```{r,echo=T}
descriptivo(datos_pca[,8])
hist(col="darkblue",datos_pca[,8],main="ZTLIBROP")
```

De nuevo ne este caso las medidas de centralidad están desviadas a la izquierda, además se denota una fuerte asimetría en la distribución.

También llama la atención como el rango de esta varuiable es mayor hacia la derecha que el de las demás.

\newpage

**ZTEJERCI** (variable con conclusiones sesgadas)

```{r,echo=T}
descriptivo(datos_pca[,9])
hist(col="darkblue",datos_pca[,9],main="ZTEJERCI")
```



En este caso tenemos unas medidas de centralidad muy desviadas a la izquierda, con una fuerte asimetría y una concentración de datos muy alta en la primera columna.

El rango de esta variable es mucho más amplio que el de todas las demás, y resulta llamativo como los últimos valores de la derecha no tienen una frecuencia alta, haciendo sospechar de que sean outliers.

\newpage

**ZTPOBACT** 

```{r,echo=T}
descriptivo(datos_pca[,10])
hist(col="darkblue",datos_pca[,10],main="ZTPOBACT")
```

Esta distribución se parece más a una normal que algunas de las estudiadas anteriormente, está más centrada (sus medidas de centralidad son más cercanas a 0) y su curtosis es baja, lo que indica una distribución aplanada.

Mirando el histograma apreciamos algunos valores cerca de -2 que pueden parecer outliers por su separación con el resto de los valores representados.

\newpage

**ZTENERGI**  

```{r,echo=T}
descriptivo(datos_pca[,11])
hist(col="darkblue",datos_pca[,11],main="ZTENERGI")
```

Al igual que en uno de los casos anteriores tenemos las medidas de centralidad desviadas a la izquierda, una curtosis bastante alta y un rango de valores amplio, con algunos separados de los demás en los extremos que parecen outliers.

**NOTAR**: el hecho de que algunas de las variables estén desplazadas hacia la derecha, o que los outliers sean en esta dirección, es debido a que estas son estandarizaciones de variables positivas, en una variable positiva el valor mas extremo inferior como mucho es $0$.

Las que si tienen extensión hacia la izquierda, es debido a que no hay un gran porcentaje de la población que se acerque al extremo inferior, entonces el $0$ realmente si acaba siendo un valor "extremo" para estas variables.

También notar que el *CVc* toma valores bastante "sin sentido" debido a que esta medida no tiene mucho sentido en variables que toman valores positivos y negativos, en el caso de la variable 15 ese valor es devido a que $Q_1 \approx -Q_3$

## Exploración Gráfica

Procedemos con los diagramas de cajas y bigotes, para una detección primaria de outliers univariantes.

```{r,echo=T}
colfunc<-colorRampPalette(c("darkblue","lightblue"))
boxplot(datos_pca,
        xlab=NULL,
        ylab="Nombre-y",
        col=colfunc(15),
        las=2)
```

Se puede observar que la mayoría de gráficos se asemejan en cierto modo a una normal,estando todos ligeramente desplazados, y destacando que POBDENS y ZTEJERCI tienen una mayor concentración de valores. La mayoría de outliers se encuentran en ZTEJERCI como cola superior.

Pese a que algunas de las distribuciones de las variables que presentan outliers no se parecen demasiado a una normal, se ha tomado la decisión de eliminarlos, a pesar de que pueda no ser lo más correcto, ya que boxplot asume que la distribución es normal y en algunos casos esto parece dudoso.

A continuación vamos a observar (de nuevo) la forma de las distribuciones de las variables mediante sus histogramas

```{r,echo=T}
par(mar=c(1,1,1,1))
par(mfrow=c(3,5))
invisible(apply(datos_pca, 2,function(x){hist(x,main=NULL,col="darkblue",xlab=NULL,ylab=NULL)}))

```

Observamos que realmente pocas de las variables realmente se podrían considerar normales con los datos que tenemos, pero son muy pocos, por lo que quizá al recoger una muestra mayor de observaciones se podrían sacar mejores conclusiones sobre las distribuciones de las variables.

### Tratamiento de outliers

Los valores outlier comentados anteriormente (según si el diagrama boxplot los consideraba outliers o no) serán intercambiados por la media. 

```{r,echo=T}
outlier<-function(data,na.rm=T){
  H<-1.5*IQR(data)
  
  if(any(data<=(quantile(data,0.25,na.rm = T)-H))){
    data[data<=quantile(data,0.25,na.rm = T)-H]<-NA
    data[is.na(data)]<-mean(data,na.rm=T)
    data<-outlier(data)}
  
  if(any(data>=(quantile(data,0.75, na.rm = T)+H))){
    data[data>=quantile(data,0.75, na.rm = T)+H]<-NA
    data[is.na(data)]<-mean(data,na.rm=T)
    data<-outlier(data)
  }
  return(data)
}

datos_pca[,-12:-13]<-apply(datos_pca[,-12:-13], 2, outlier)
```

Una vez tratados los outliers, vemos de nuevo los gráficos boxplot.

```{r,echo=T}
boxplot(datos_pca,
        xlab=NULL,
        ylab="Nombre-y",
        col=colfunc(15),
        las=2)
```
 
Así comprobamos que hemos eliminado los outliers.

\newpage

### Normalidad

Para poder aplicar ciertas técnicas estadísticas, es importante saber si estamos tratando con variables normales, para ello usaremos el método gráfico *qqplot*.

```{r,echo=T}
par(mar=c(1,1,1,1))
par(mfrow=c(3,5))
invisible(apply(datos_pca, 2, function(x){
  qqnorm(x,main=NULL)
  abline(a=0,b=1,col="red")
}))
```

Vemos como casi todas las variables salvo la novena se ajustan bastante bien a la normalidad, siendo las cuarta y décima las que mejor se acercan a ella.

En este caso no tomaremos medidas para obtener normalidad en los datos.

\newpage


### Homocedasticidad

La homocedasticidad se debe comparar dentro de una misma variable, para dos o mas grupos diferenciados; en el caso de este dataset, podemos comprobar si existe homocedasticidad entre los grupos definidos por sus continentes.

Comenzamos añadiendo a los datos completos la variable continente:

```{r,echo=T}
datos_enteros$continente <- c(
  "africa", "africa", "america", "oceania",
  "america", "america", "america", "asia",
  "asia", "africa", "europa", "asia", 
  "europa", "europa", "asia", "asia",
  "asia", "asia", "europa", "asia", "asia",
  "africa", "america", "africa", "asia", "europa",
  "europa", "europa", "europa", "europa",
  "europa", "asia", "america", "asia")
```

Comprobamos ahora la homocedasticidad:

```{r,echo=T}
ind<-which(datos_enteros$continente=="europa"|datos_enteros$continente=="asia"|datos_enteros$continente=="africa")
factores<-datos_enteros$continente[ind]
#Como se han eliminado los valores outlier, usamos con centro la media en vez de la mediana
#H0:homocedasticidad
apply(datos_pca[ind,], 2, function(x){
  if(leveneTest(x,as.factor(factores),center=median)$"Pr(>F)"[1]>0.05){
    "Existe homocedasticidad entre los grupos"
  }
  else{"No existe homocedasticidad entre los grupos"}
  })
```

Vemos como para la mayoría de variables, seccionando los paises según continente, se tiene la misma varianza. Donde no se cumple es para el índice de ZESPVIDA y ZPOBURB. 

\newpage

## Exploración Descriptiva
    
A continuación vamos a sacar los principales estadísticos descriptivos ahora que tenemos los datos transformados.

**Originales**
```{r,echo=T}
summary(datos)
```

**Tratados**
```{r,echo=T}
summary(datos_pca)
```

\section{Análisis exploratorio multivariante}

\subsection{Estudiando los datos}

# Supuestos de correlación

Comenzamos comprobando si existe correlación entre las variables:

```{r,echo=T}
cor(datos_pca)
```

El contraste de esfericidad de Bartlett permite comprobar si las correlaciones son distintas de 0 de modo significativo. La hipótesis nula es que det(R)=1. La función "cortest.bartlett" del paquete "pysch" realiza este test. Esta función trabaja con datos normalizados.

```{r, echo=T}
library(psych)
# Se normalizan los datos
datos_normalizados<-scale(datos_pca)
# Se hace el test de esfericidad
cortest.bartlett(cor(datos_normalizados))
```
Para estos datos se obtiene un test significativo de modo que se rechaza la  hipótesis nula y por tanto los datos no están incorrelados. Aún así, seguiremos adelante y si algo no funciona puede deberse a esto.

# Análisis exploratorio de los datos

Como ya hemos imputado los valores perdidos y eliminado los outliers no hace falta repetir el procedimiento.

# Estudio de posibilidad de reducción de la dimensión

Vamos a realizar un análisis de componentes principales (PCA). La función "prcomp" del paquete base de R realiza este análisis.

Pasamos los parámetros "scale" y "center" a TRUE para considerar los datos originales normalizados. Además, el campo "rotation" del objeto PCA es una matriz cuyas columnas son los coeficientes de las componentes principales. Finalmente, en el campo "sdev" del mismo objeto obtenemos la información sobre desviaciones típicas de cada componente principal, proporción de varianza explicada y acumulada.


````{r,echo=T}
PCA<-prcomp(datos_pca, scale=T, center = T)
PCA$rotation
PCA$sdev
summary(PCA)

```
Representamos ahora la proporción de varianza explicada y acumulada:

```{r, echo=T}
library("ggplot2")

# El siguiente gráfico muestra la proporción de varianza explicada
varianza_explicada <- PCA$sdev^2 / sum(PCA$sdev^2)
ggplot(data = data.frame(varianza_explicada, pc = 1:11),
       aes(x = pc, y = varianza_explicada, fill=varianza_explicada )) +
  geom_col(width = 0.3) +
  scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
  labs(x = "Componente principal", y= " Proporción de varianza explicada")

# El siguiente gráfico muestra la proporción de varianza explicada
varianza_acum<-cumsum(varianza_explicada)
ggplot( data = data.frame(varianza_acum, pc = 1:11),
        aes(x = pc, y = varianza_acum ,fill=varianza_acum )) +
  geom_col(width = 0.5) +
  scale_y_continuous(limits = c(0,1)) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Proporción varianza acumulada")

```

Buscamos ahora el número de componentes principales óptimo. En este caso vamos a utilizar la regla de Abdi et al. (2010). Se promedian las varianzas explicadas por la componentes principales y se seleccionan aquellas cuya proporción de  varianza explicada supera la media.
# En este caso se eligen tan solo cuatro direcciones principales tal y como se puede ver
# que acumulan casi un 80% de varianza explicada

```{r, echo=T}
PCA$sdev^2
mean(PCA$sdev^2)
```
En este caso la media es 1, luego se escogerían la primera, segunda y tercera. Esto son, 3 componentes principales.

Ahora vamos a representar las componentes principales, realizando una comparativa entre la primera y segunda componente principal analizando  qué variables tienen más peso para la definición de cada componente principal, después entre la primera y la tercera y finalmente entre la segunda y tercera.

```{r, echo=T}
library("factoextra")

fviz_pca_var(PCA,
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()

fviz_pca_var(PCA,axes=c(1,3),
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()

fviz_pca_var(PCA,axes=c(2,3),
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()
```
Es posible también representar las observaciones de los objetos junto con las componentes  principales mediante la orden "contrib" de la función "fviz_pca_ind", así como identificar con colores aquellas observaciones que mayor varianza explican de las componentes principales.


```{r, echo=T}
# Observaciones en la primera y segunda componente principal
fviz_pca_ind(PCA,col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var")+theme_bw()

# Observaciones en la primera y tercera componente principal
fviz_pca_ind(PCA,axes=c(1,3),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var")+theme_bw()

# Observaciones en la segunda y tercera componente principal
fviz_pca_ind(PCA,axes=c(2,3),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var")+theme_bw()
```
 
 Además haremos una representación conjunta de variables y observaciones que relaciona visualmente las posibles relaciones entre las observaciones, las contribuciones de los individuos a las varianzas de las componentes y el peso de las variables en cada componentes principal.
 
```{r, echo=T}
# Variables y observaciones en las 1?  y 2? componente principal
fviz_pca(PCA,
         alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE,
         legend.title="Distancia")+theme_bw()

# Variables y observaciones en las 1?  y 3? componente principal
fviz_pca(PCA,axes=c(1,3),
         alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE,
         legend.title="Distancia")+theme_bw()

# Variables y observaciones en las 2  y 3? componente principal
fviz_pca(PCA,axes=c(2,3),
         alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE,
         legend.title="Distancia")+theme_bw()
```
Por último, ya que el objeto de este estudio es reducir la dimensión de las variables utilizadas, es posible obtener las coordenadas de los datos originales tipificados en el nuevo sistema de referencia. De hecho lo tenemos almacenado desde que utilizamos la función prcomp para crear la variable PCA

```{r,echo=T}

head(PCA$x,n=3)
```


# Reducción de dimensión mediante variables latentes

Como ya hemos comprobado las correlaciones y test de esfericidad anteriormente, nos saltaremos ese paso.

Vamos a comparar las salidas con el método del factor principal y con el de máxima verosimilitud.

```{r, echo=T}
library("polycor")

poly_cor<-hetcor(datos_pca)$correlations

modelo1<-fa(poly_cor,
            nfactors = 3,
            rotate = "none",
            fm="mle") # modelo máxima verosimilitud

modelo2<-fa(poly_cor,
            nfactors = 3,
            rotate = "none",
            fm="minres") # modelo minimo residuo

```

Comenzamos comparando las comunalidades:

```{r, echo=T}
# Comparacion comunalidades
sort(modelo1$communality,decreasing = T)->c1
sort(modelo2$communality,decreasing = T)->c2
head(cbind(c1,c2))
```
Ahora comparamos las unicidades, es decir la proporción de varianza que no ha sido explicada por el factor (1-comunalidad)

```{r, echo=T}
sort(modelo1$uniquenesses,decreasing = T)->u1
sort(modelo2$uniquenesses,decreasing = T)->u2
head(cbind(u1,u2))
```
Determinemos ahora el número óptimo de factores.
Hay diferentes criterios, entre los que destacan el Scree plot (Cattel 1966) y el análisis paralelo (Horn 1965).

```{r, echo=T}
scree(poly_cor)
fa.parallel(poly_cor,n.obs=200,fa="fa",fm="minres")
```

Se deduce que el número óptimo de factores es 3, como ya vimos en el apartado anterior.

Estimamos el modelo factorial con 3 factores implementando una rotacion tipo varimax para buscar una interpretación más simple y mostramos la matriz de pesos factorial rotada.

```{r, echo=T}
modelo_varimax<-fa(poly_cor,nfactors = 3,rotate = "varimax",
                   fa="mle")
print(modelo_varimax$loadings,cut=0) 
```

Para verlo más claro vamos a representarlo:

```{r, echo=T}
fa.diagram(modelo_varimax)
```
En este diagrama se ve que el primer factor está asociado con ZPSERVI, ZPAGRICU, ZPOBURB, ZESPVIDA, ZTMINFAN, ZTLIBROP y ZTMEDICO. mientras la segunda es con ZTPOBACT y ZTENERGI y la última con ZTEJERCI.

Finalmente comprobamos que en efecto el número de factores elegido es suficiente.

```{r,echo=T}
library(stats)
factanal(datos_pca,factors=3, rotation="none")

```

Con ese p-valor para cierto nivel de confianza se acepta la hipótesis y por tanto el número de factores es suficiente.


# Análisis de la normalidad multivariante

A continuación hacemos una exploración gráfica de la normalidad de las distribuciones individuales de nuestros predictores representando los histogramas y los gráficos qqplot.

Como en el apartado univariante ya hemos comprobado la normalidad de cada una de las variables no vamos a repetirlo.

El paquete MVN contiene funciones que permiten realizar los tres test que se utilizan habitualmente para contrastar la normalidad multivariante.
Esta normalidad multivariante puede verse afectada por la presencia de outliers. En este paquete también encontramos funciones para el análisis de outliers.

```{r}
library(MVN)
outliers <- mvn(data = datos_pca, mvnTest = "hz", multivariateOutlierMethod = "quan")
```

Se detectan 11 outliers en las observaciones. Aunque considerar outliers en 11 observaciones de 33 puede deberse a la falta de datos, y más cuando los outliers ya se eliminaron varios en el análisis univariante. Sin embargo solo de los dos test realizados a continuación encuentran evidencias al 5% de significación de falta de normalidad multivariante.

```{r}

royston_test <- mvn(data = datos_pca, mvnTest = "royston", multivariatePlot = "qq")

royston_test$multivariateNormality

hz_test <- mvn(data = datos_pca, mvnTest = "hz")
hz_test$multivariateNormality

```

Pese a que uno de los test ha rechazado que siga una normal multivariante, vamos a hacerle caso al otro test y seguir, ya que probablemente esto se deba al bajo tamaño de observaciones con el que contamos y en el gráfico los valores no se alejan demasiado de la recta.

# Clasificación

Como acabamos de comprobar la noramlidad multivariante y ya habíamos comprobado la univariante, solo falta la homogeneidad de la varianza.

## Homogeneidad de la varianza

Usaremos el test de Box M, que es una extensión del de Barttlet para escenarios multivariantes. Hay que tener en cuenta que es muy sensible a que los datos efectivamente se distribuyan según una normal multivariante. Por este motivo se recomienda utilizar una significación (p-value) <0.001.

La hipóstesis nula a contrastar es la de igualdad de matrices de covarianzas en todos los grupos.

```{r}
library(biotools)
#datos_enteros<-read.spss("DB_3.sav", to.data.frame = TRUE)
datos_enteros[15:34,1]<-"china"
datos_enteros[1:15,1]<-"aa"
datos_enteros$PAIS <- as.factor(datos_enteros$PAIS)
col_names <- names(datos_enteros[,2:12])
# to do it for some names in a vector named 'col_names'
datos_enteros[col_names] <- lapply(datos_enteros[col_names] , as.double)
datos_enteros[,1]
boxM(data = datos_enteros[, 2:12], grouping = datos_enteros[, 1])
```

En este caso no rechazamos la hipótesis nula ya que **p-value = 0.132 > 0.001** y por tanto **asumimos la homogeneidad de varianzas**.
Es importante recordar que para que esta **conclusión sea fiable** debe darse el supuesto de **normalidad multivariante**.

## Mediante análisis discriminante lineal

La función lda del paquete MASS realiza la clasificación.

```{r}
library(MASS)
continente <- datos_enteros$continente
continente <- as.factor(continente)

modelo_lda <- lda(formula = continente ~ ZPOBDENS + ZTMINFAN, data = datos_enteros[,-1])
modelo_lda
```

La salida de este objeto, nos muestra las probabilidades a priori de cada grupo.

Ahora podemos realizar predicciones:

```{r, echo=T}
nuevas_observaciones <- data.frame(ZPOBDENS = -0.5, ZTMINFAN = 0.7)
predict(object = modelo_lda, newdata = nuevas_observaciones)
```
### Validación

```{r}
library(biotools)

pred <- predict(modelo_lda, dimen = 1)
confusionmatrix(datos_enteros$continente, pred$class)

# Porcentaje de errores de clasificación
trainig_error <- mean(datos_enteros$continente != pred$class) * 100
paste("trainig_error=", trainig_error, "%")
```
El error con los datos de training es del 52%, lo cuál es bastante alto.

### Visualización

```{r,echo=T}
library(klaR)
continente <- datos_enteros$continente
continente <- as.factor(continente)
partimat(continente ~ ZPOBDENS + ZTMINFAN,
         data = datos_pca, method = "lda", prec = 200,
         col.mean = "firebrick",nplots.vert =1, nplots.hor=3)
```


## Mediante análisis discriminante cuadrático

Realizamos ahora análogamente para el modelo cuadrático.

```{r}
library(MASS)

#modelo_qda <- qda(formula = continente ~ ZPOBDENS + ZTMINFAN, data = datos_enteros[,-1])
#modelo_qda
```

La salida de este objeto, nos muestra las probabilidades a priori de cada grupo.


# Clustering

```{r, echo=T}
library(tidyverse)
library(cluster)
library(factoextra)
```

Vamos a realizar clustering aplicando kmeans, un método no jerárquico de agrupamiento bastante robusto.

Como ya hemos tratado los valores perdidos y outliers vamos simplemente a normalizar los datos:

```{r, echo=T}
# Para evitar que el análisis cluster se vea influido por cualquier variable arbitraria se estandarizan los datos
datos_pca<-scale(datos_pca)
head(datos_pca)
```

Aplicamos ahora clustering con 4 clusters:
```{r}
k4 <- kmeans(datos_pca, centers = 4, nstart = 25)
str(k4)
```

La salida que proporciona la función kmeans es una lista de información, de la que destacan las siguientes:

* *cluster*: es un vector de enteros, de 1 a K, que indica el cluster en el que ha sido ubicado cada observación.
* *centers*: una matriz con los sucesivos centros de los clusters.
* *totss*: la suma total de cuadrados.
* *withinss*: vector de suma de cuadrados dentro del cluster (un componente por cluster).
* *tot.withinss*: suma total de cuadrados dentro de conglomerados, i.e. sum(withinss).
* *betweens*: suma de cuadrados entre grupos, i.e. totss-tot.withinss.
* *size*: el número de observaciones en cada cluster.

```{r}
k4
```

Al mostrar la variable k4 se ve como las agrupaciones dieron como resultado tamaños realtivamente equilibrados (6, 15, 7 y 6). También se ven los centros de cada grupo (medias) en todas las variables. Y por último la asignación de grupo para cada observación por ejemplo, africasu se asignó al 3, Argelia al 1, etc.

Una forma visual de resumir los resultados de forma elegante y con una interpretación directa es mediante el uso de la función *fviz_cluster*.

```{r}
fviz_cluster(k4,data=datos_pca)
```

Apreciamos que los clusters 1 y 4 se solapan entre ellos ligeramente, aunque puede ser culpa de la proyección. También parece que el cluster 4 está formado por varios outliers, mientras los otros parecen más compactos.

## Buscando el número óptimo de clusters

Buscamos el número de clusters que maximice la similaridad intracluster y minimice la similaridad intercluster. Como kmeans recibe como argumento el número de clusters a realizar, debemos buscar nosotros cuál será el mejor número de clusters.

Para ello vamos a usar el Método de Silhouette, ya que se basa en la medida de calidad de agrupamiento Silhouette, que es ampliamente utilizada. El número óptimo de clusters según este enfoque es, de entre un rango de valores posibles para k, aquel que maximiza la silueta promedio.

```{r}
set.seed(123)

fviz_nbclust(datos_pca, kmeans, method = "silhouette")
```

Vemos un pico considerable en el valor 2, lo que parece razonable ya que contamos con pocos datos. Repetimos ahora el clustering realizado antes fijando como número de clusters 2.

```{r}
k2 <- kmeans(datos_pca, centers = 2, nstart = 123)
str(k2)

k2

fviz_cluster(k2,data=datos_pca)
```

Observamos al realizar esta agrupación dos clusters bien diferenciados, que no se solapan y de tamaños casi idénticos (18 y 16).

Además los centros no parecen demasiado influenciados por algún valor concreto (outlier). También destaca como las medias de las variables en cada cluster están muy diferenciadas, ya que en todas ellas una tiene media postiva y la otra negativa, de forma que se comprueba que en efecto los valores de cada variable (y más en conjunto) son muy diferentes entre los clusters.









 



