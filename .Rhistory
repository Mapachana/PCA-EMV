data[is.na(data)]<-mean(data,na.rm=T)
data
}
datos_pca<-as.data.frame(apply(datos, 2, not_available))
cbind(apply(is.na(datos_pca),2,sum),apply(is.na(datos_pca),2,sum)/dim(datos_pca)[1])
#Definimos las medidas resistentes
PMC<-function(x){ return((as.double(quantile(x,0.25))+as.double(quantile(x,0.75)))/2)}
trimedia<-function(x){return((median(x)+PMC(x))/2)}
centrimedia<-function(x){
indices<-(x>quantile(x,0.25)&x<quantile(x,0.75))
valores<-x[indices]
return(sum(valores)/length(valores))
}
RIQ<-function(x){return(quantile(x,0.75)-quantile(x,0.25))}
MEDA<-function(x){return(median(abs(x-median(x))))}
CVc<-function(x){return((quantile(x,0.75)-quantile(x,0.25))/(quantile(x,0.75)+quantile(x,0.25)))}
H1<-function(x){return((quantile(x,0.25)+quantile(x,0.75)-2*median(x))/(2*median(x)))}
H2<-function(x){return(median(x)-(quantile(x,0.1)+quantile(x,0.9))/(2))}
H3<-function(x){return(H2(x)/median(x))}
#Creamos una función que aplique todas estas medidas
descriptivo<-function(x){
temp<-rbind(PMC(x),trimedia(x),centrimedia(x))
rownames(temp)<-c("PMC","Trimedia","Centrimedia")
centralidad<-list(clasica=list(media=mean(x)),resistente=temp)
temp<-rbind(RIQ(x),MEDA(x),CVc(x))
rownames(temp)<-c("Rango Inter-Cuartílico","MEDA","CVc")
dispersion<-list(clasica=list(desviación_típica=sd(x),Coef_varización=sd(x)/mean(x),rango=range(x)),resistente=temp)
temp<-rbind(H1(x),H2(x),H3(x))
rownames(temp)<-c("Asimetría de Yule","Asimetría de Kelly","Asimetría de Kelly adimensional")
forma<-list(clasica=list(skewness=skewness(x),kurtosis=kurtosis(x)),resistente=temp)
cat(names(x))
return(list(centralidad=centralidad,dispersion=dispersion,forma=forma))
}
descriptivo(datos_pca[,1])
hist(col="darkblue",datos_pca[,1],main="ZPOBDENS")
descriptivo(datos_pca[,2])
hist(col="darkblue",datos_pca[,2],main="ZMINFAN")
descriptivo(datos_pca[,3])
hist(col="darkblue",datos_pca[,3],main="ZESPVIDA")
descriptivo(datos_pca[,4])
hist(col="darkblue",datos_pca[,4],main="ZPOBURB")
descriptivo(datos_pca[,5])
hist(col="darkblue",datos_pca[,5],main="ZTMEDICO")
descriptivo(datos_pca[,6])
hist(col="darkblue",datos_pca[,6],main="ZPAGRICU")
descriptivo(datos_pca[,7])
hist(col="darkblue",datos_pca[,7],main="ZPSERVI")
descriptivo(datos_pca[,8])
hist(col="darkblue",datos_pca[,8],main="ZTLIBROP")
descriptivo(datos_pca[,9])
hist(col="darkblue",datos_pca[,9],main="ZTEJERCI")
descriptivo(datos_pca[,10])
hist(col="darkblue",datos_pca[,10],main="ZTPOBACT")
descriptivo(datos_pca[,11])
hist(col="darkblue",datos_pca[,11],main="ZTENERGI")
colfunc<-colorRampPalette(c("darkblue","lightblue"))
boxplot(datos_pca,
xlab=NULL,
ylab="Nombre-y",
col=colfunc(15),
las=2)
par(mar=c(1,1,1,1))
par(mfrow=c(3,5))
invisible(apply(datos_pca, 2,function(x){hist(x,main=NULL,col="darkblue",xlab=NULL,ylab=NULL)}))
outlier<-function(data,na.rm=T){
H<-1.5*IQR(data)
if(any(data<=(quantile(data,0.25,na.rm = T)-H))){
data[data<=quantile(data,0.25,na.rm = T)-H]<-NA
data[is.na(data)]<-mean(data,na.rm=T)
data<-outlier(data)}
if(any(data>=(quantile(data,0.75, na.rm = T)+H))){
data[data>=quantile(data,0.75, na.rm = T)+H]<-NA
data[is.na(data)]<-mean(data,na.rm=T)
data<-outlier(data)
}
return(data)
}
datos_pca[,-12:-13]<-apply(datos_pca[,-12:-13], 2, outlier)
boxplot(datos_pca,
xlab=NULL,
ylab="Nombre-y",
col=colfunc(15),
las=2)
par(mar=c(1,1,1,1))
par(mfrow=c(3,5))
invisible(apply(datos_pca, 2, function(x){
qqnorm(x,main=NULL)
abline(a=0,b=1,col="red")
}))
datos_enteros$continente <- c(
"africa", "africa", "america", "oceania",
"america", "america", "america", "asia",
"asia", "africa", "europa", "asia",
"europa", "europa", "asia", "asia",
"asia", "asia", "europa", "asia", "asia",
"africa", "america", "africa", "asia", "europa",
"europa", "europa", "europa", "europa",
"europa", "asia", "america", "asia")
ind<-which(datos_enteros$continente=="europa"|datos_enteros$continente=="asia"|datos_enteros$continente=="africa")
factores<-datos_enteros$continente[ind]
#Como se han eliminado los valores outlier, usamos con centro la media en vez de la mediana
#H0:homocedasticidad
apply(datos_pca[ind,], 2, function(x){
if(leveneTest(x,as.factor(factores),center=median)$"Pr(>F)"[1]>0.05){
"Existe homocedasticidad entre los grupos"
}
else{"No existe homocedasticidad entre los grupos"}
})
summary(datos)
summary(datos_pca)
cor(datos_pca)
library(psych)
# Se normalizan los datos
datos_normalizados<-scale(datos_pca)
# Se hace el test de esfericidad
cortest.bartlett(cor(datos_normalizados))
PCA<-prcomp(datos_pca, scale=T, center = T)
PCA$rotation
PCA$sdev
summary(PCA)
library("ggplot2")
# El siguiente gr?fico muestra la proporci?n de varianza explicada
varianza_explicada <- PCA$sdev^2 / sum(PCA$sdev^2)
ggplot(data = data.frame(varianza_explicada, pc = 1:11),
aes(x = pc, y = varianza_explicada, fill=varianza_explicada )) +
geom_col(width = 0.3) +
scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
labs(x = "Componente principal", y= " Proporci?n de varianza explicada")
# El siguiente gr?fico muestra la proporci?n de varianza explicada
varianza_acum<-cumsum(varianza_explicada)
ggplot( data = data.frame(varianza_acum, pc = 1:11),
aes(x = pc, y = varianza_acum ,fill=varianza_acum )) +
geom_col(width = 0.5) +
scale_y_continuous(limits = c(0,1)) +
theme_bw() +
labs(x = "Componente principal",
y = "Proporci?n varianza acumulada")
PCA$sdev^2
mean(PCA$sdev^2)
library("factoextra")
fviz_pca_var(PCA,
repel=TRUE,col.var="cos2",
legend.title="Distancia")+theme_bw()
fviz_pca_var(PCA,axes=c(1,3),
repel=TRUE,col.var="cos2",
legend.title="Distancia")+theme_bw()
fviz_pca_var(PCA,axes=c(2,3),
repel=TRUE,col.var="cos2",
legend.title="Distancia")+theme_bw()
# Observaciones en la primera y segunda componente principal
fviz_pca_ind(PCA,col.ind = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel=TRUE,legend.title="Contrib.var")+theme_bw()
# Observaciones en la primera y tercera componente principal
fviz_pca_ind(PCA,axes=c(1,3),col.ind = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel=TRUE,legend.title="Contrib.var")+theme_bw()
# Observaciones en la segunda y tercera componente principal
fviz_pca_ind(PCA,axes=c(2,3),col.ind = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel=TRUE,legend.title="Contrib.var")+theme_bw()
# Variables y observaciones en las 1?  y 2? componente principal
fviz_pca(PCA,
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
# Variables y observaciones en las 1?  y 3? componente principal
fviz_pca(PCA,axes=c(1,3),
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
# Variables y observaciones en las 2  y 3? componente principal
fviz_pca(PCA,axes=c(2,3),
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
head(PCA$x,n=3)
library("polycor")
poly_cor<-hetcor(datos_pca)$correlations
modelo1<-fa(poly_cor,
nfactors = 3,
rotate = "none",
fm="mle") # modelo máxima verosimilitud
modelo2<-fa(poly_cor,
nfactors = 3,
rotate = "none",
fm="minres") # modelo minimo residuo
# Comparacion comunalidades
sort(modelo1$communality,decreasing = T)->c1
sort(modelo2$communality,decreasing = T)->c2
head(cbind(c1,c2))
sort(modelo1$uniquenesses,decreasing = T)->u1
sort(modelo2$uniquenesses,decreasing = T)->u2
head(cbind(u1,u2))
scree(poly_cor)
fa.parallel(poly_cor,n.obs=200,fa="fa",fm="minres")
modelo_varimax<-fa(poly_cor,nfactors = 3,rotate = "varimax",
fa="mle")
print(modelo_varimax$loadings,cut=0)
fa.diagram(modelo_varimax)
library(stats)
factanal(datos_pca,factors=3, rotation="none")
library(MVN)
outliers <- mvn(data = datos_pca, mvnTest = "hz", multivariateOutlierMethod = "quan")
royston_test <- mvn(data = datos_pca, mvnTest = "royston", multivariatePlot = "qq")
royston_test$multivariateNormality
hz_test <- mvn(data = datos_pca, mvnTest = "hz")
hz_test$multivariateNormality
library(biotools)
#datos_enteros<-read.spss("DB_3.sav", to.data.frame = TRUE)
datos_enteros[15:34,1]<-"china"
datos_enteros[1:15,1]<-"aa"
datos_enteros$PAIS <- as.factor(datos_enteros$PAIS)
col_names <- names(datos_enteros[,2:12])
# to do it for some names in a vector named 'col_names'
datos_enteros[col_names] <- lapply(datos_enteros[col_names] , as.double)
datos_enteros[,1]
boxM(data = datos_enteros[, 2:12], grouping = datos_enteros[, 1])
library(MASS)
continente <- datos_enteros$continente
continente <- as.factor(continente)
modelo_lda <- lda(formula = continente ~ ZPOBDENS + ZTMINFAN, data = datos_enteros[,-1])
modelo_lda
nuevas_observaciones <- data.frame(ZPOBDENS = -0.5, ZTMINFAN = 0.7)
predict(object = modelo_lda, newdata = nuevas_observaciones)
library(biotools)
pred <- predict(modelo_lda, dimen = 1)
confusionmatrix(datos_enteros$continente, pred$class)
# Porcentaje de errores de clasificación
trainig_error <- mean(datos_enteros$continente != pred$class) * 100
paste("trainig_error=", trainig_error, "%")
library(klaR)
continente <- datos_enteros$continente
continente <- as.factor(continente)
partimat(continente ~ ZPOBDENS + ZTMINFAN,
data = datos_pca, method = "lda", prec = 200,
col.mean = "firebrick",nplots.vert =1, nplots.hor=3)
library(MASS)
modelo_qda <- qda(formula = continente ~ ZPOBDENS + ZTMINFAN, data = datos_enteros[,-1])
# install.packages("EnvStats")
library(EnvStats)
library(car)
library(foreign)
getwd()
datos_enteros<-read.spss("DB_3.sav", to.data.frame = TRUE)
#Cogemos las variables de interes, es decir, elimino las etiquetas del pais
datos<-datos_enteros[,2:12]
head(datos)
cbind(apply(is.na(datos),2,sum),apply(is.na(datos),2,sum)/dim(datos)[1])
not_available<-function(data,na.rm=F){
data[is.na(data)]<-mean(data,na.rm=T)
data
}
datos_pca<-as.data.frame(apply(datos, 2, not_available))
cbind(apply(is.na(datos_pca),2,sum),apply(is.na(datos_pca),2,sum)/dim(datos_pca)[1])
#Definimos las medidas resistentes
PMC<-function(x){ return((as.double(quantile(x,0.25))+as.double(quantile(x,0.75)))/2)}
trimedia<-function(x){return((median(x)+PMC(x))/2)}
centrimedia<-function(x){
indices<-(x>quantile(x,0.25)&x<quantile(x,0.75))
valores<-x[indices]
return(sum(valores)/length(valores))
}
RIQ<-function(x){return(quantile(x,0.75)-quantile(x,0.25))}
MEDA<-function(x){return(median(abs(x-median(x))))}
CVc<-function(x){return((quantile(x,0.75)-quantile(x,0.25))/(quantile(x,0.75)+quantile(x,0.25)))}
H1<-function(x){return((quantile(x,0.25)+quantile(x,0.75)-2*median(x))/(2*median(x)))}
H2<-function(x){return(median(x)-(quantile(x,0.1)+quantile(x,0.9))/(2))}
H3<-function(x){return(H2(x)/median(x))}
#Creamos una función que aplique todas estas medidas
descriptivo<-function(x){
temp<-rbind(PMC(x),trimedia(x),centrimedia(x))
rownames(temp)<-c("PMC","Trimedia","Centrimedia")
centralidad<-list(clasica=list(media=mean(x)),resistente=temp)
temp<-rbind(RIQ(x),MEDA(x),CVc(x))
rownames(temp)<-c("Rango Inter-Cuartílico","MEDA","CVc")
dispersion<-list(clasica=list(desviación_típica=sd(x),Coef_varización=sd(x)/mean(x),rango=range(x)),resistente=temp)
temp<-rbind(H1(x),H2(x),H3(x))
rownames(temp)<-c("Asimetría de Yule","Asimetría de Kelly","Asimetría de Kelly adimensional")
forma<-list(clasica=list(skewness=skewness(x),kurtosis=kurtosis(x)),resistente=temp)
cat(names(x))
return(list(centralidad=centralidad,dispersion=dispersion,forma=forma))
}
descriptivo(datos_pca[,1])
hist(col="darkblue",datos_pca[,1],main="ZPOBDENS")
descriptivo(datos_pca[,2])
hist(col="darkblue",datos_pca[,2],main="ZMINFAN")
descriptivo(datos_pca[,3])
hist(col="darkblue",datos_pca[,3],main="ZESPVIDA")
descriptivo(datos_pca[,4])
hist(col="darkblue",datos_pca[,4],main="ZPOBURB")
descriptivo(datos_pca[,5])
hist(col="darkblue",datos_pca[,5],main="ZTMEDICO")
descriptivo(datos_pca[,6])
hist(col="darkblue",datos_pca[,6],main="ZPAGRICU")
descriptivo(datos_pca[,7])
hist(col="darkblue",datos_pca[,7],main="ZPSERVI")
descriptivo(datos_pca[,8])
hist(col="darkblue",datos_pca[,8],main="ZTLIBROP")
descriptivo(datos_pca[,9])
hist(col="darkblue",datos_pca[,9],main="ZTEJERCI")
descriptivo(datos_pca[,10])
hist(col="darkblue",datos_pca[,10],main="ZTPOBACT")
descriptivo(datos_pca[,11])
hist(col="darkblue",datos_pca[,11],main="ZTENERGI")
colfunc<-colorRampPalette(c("darkblue","lightblue"))
boxplot(datos_pca,
xlab=NULL,
ylab="Nombre-y",
col=colfunc(15),
las=2)
par(mar=c(1,1,1,1))
par(mfrow=c(3,5))
invisible(apply(datos_pca, 2,function(x){hist(x,main=NULL,col="darkblue",xlab=NULL,ylab=NULL)}))
outlier<-function(data,na.rm=T){
H<-1.5*IQR(data)
if(any(data<=(quantile(data,0.25,na.rm = T)-H))){
data[data<=quantile(data,0.25,na.rm = T)-H]<-NA
data[is.na(data)]<-mean(data,na.rm=T)
data<-outlier(data)}
if(any(data>=(quantile(data,0.75, na.rm = T)+H))){
data[data>=quantile(data,0.75, na.rm = T)+H]<-NA
data[is.na(data)]<-mean(data,na.rm=T)
data<-outlier(data)
}
return(data)
}
datos_pca[,-12:-13]<-apply(datos_pca[,-12:-13], 2, outlier)
boxplot(datos_pca,
xlab=NULL,
ylab="Nombre-y",
col=colfunc(15),
las=2)
par(mar=c(1,1,1,1))
par(mfrow=c(3,5))
invisible(apply(datos_pca, 2, function(x){
qqnorm(x,main=NULL)
abline(a=0,b=1,col="red")
}))
datos_enteros$continente <- c(
"africa", "africa", "america", "oceania",
"america", "america", "america", "asia",
"asia", "africa", "europa", "asia",
"europa", "europa", "asia", "asia",
"asia", "asia", "europa", "asia", "asia",
"africa", "america", "africa", "asia", "europa",
"europa", "europa", "europa", "europa",
"europa", "asia", "america", "asia")
ind<-which(datos_enteros$continente=="europa"|datos_enteros$continente=="asia"|datos_enteros$continente=="africa")
factores<-datos_enteros$continente[ind]
#Como se han eliminado los valores outlier, usamos con centro la media en vez de la mediana
#H0:homocedasticidad
apply(datos_pca[ind,], 2, function(x){
if(leveneTest(x,as.factor(factores),center=median)$"Pr(>F)"[1]>0.05){
"Existe homocedasticidad entre los grupos"
}
else{"No existe homocedasticidad entre los grupos"}
})
summary(datos)
summary(datos_pca)
cor(datos_pca)
library(psych)
# Se normalizan los datos
datos_normalizados<-scale(datos_pca)
# Se hace el test de esfericidad
cortest.bartlett(cor(datos_normalizados))
PCA<-prcomp(datos_pca, scale=T, center = T)
PCA$rotation
PCA$sdev
summary(PCA)
library("ggplot2")
# El siguiente gr?fico muestra la proporci?n de varianza explicada
varianza_explicada <- PCA$sdev^2 / sum(PCA$sdev^2)
ggplot(data = data.frame(varianza_explicada, pc = 1:11),
aes(x = pc, y = varianza_explicada, fill=varianza_explicada )) +
geom_col(width = 0.3) +
scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
labs(x = "Componente principal", y= " Proporci?n de varianza explicada")
# El siguiente gr?fico muestra la proporci?n de varianza explicada
varianza_acum<-cumsum(varianza_explicada)
ggplot( data = data.frame(varianza_acum, pc = 1:11),
aes(x = pc, y = varianza_acum ,fill=varianza_acum )) +
geom_col(width = 0.5) +
scale_y_continuous(limits = c(0,1)) +
theme_bw() +
labs(x = "Componente principal",
y = "Proporci?n varianza acumulada")
PCA$sdev^2
mean(PCA$sdev^2)
library("factoextra")
fviz_pca_var(PCA,
repel=TRUE,col.var="cos2",
legend.title="Distancia")+theme_bw()
fviz_pca_var(PCA,axes=c(1,3),
repel=TRUE,col.var="cos2",
legend.title="Distancia")+theme_bw()
fviz_pca_var(PCA,axes=c(2,3),
repel=TRUE,col.var="cos2",
legend.title="Distancia")+theme_bw()
# Observaciones en la primera y segunda componente principal
fviz_pca_ind(PCA,col.ind = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel=TRUE,legend.title="Contrib.var")+theme_bw()
# Observaciones en la primera y tercera componente principal
fviz_pca_ind(PCA,axes=c(1,3),col.ind = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel=TRUE,legend.title="Contrib.var")+theme_bw()
# Observaciones en la segunda y tercera componente principal
fviz_pca_ind(PCA,axes=c(2,3),col.ind = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel=TRUE,legend.title="Contrib.var")+theme_bw()
# Variables y observaciones en las 1?  y 2? componente principal
fviz_pca(PCA,
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
# Variables y observaciones en las 1?  y 3? componente principal
fviz_pca(PCA,axes=c(1,3),
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
# Variables y observaciones en las 2  y 3? componente principal
fviz_pca(PCA,axes=c(2,3),
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
head(PCA$x,n=3)
library("polycor")
poly_cor<-hetcor(datos_pca)$correlations
modelo1<-fa(poly_cor,
nfactors = 3,
rotate = "none",
fm="mle") # modelo máxima verosimilitud
modelo2<-fa(poly_cor,
nfactors = 3,
rotate = "none",
fm="minres") # modelo minimo residuo
# Comparacion comunalidades
sort(modelo1$communality,decreasing = T)->c1
sort(modelo2$communality,decreasing = T)->c2
head(cbind(c1,c2))
sort(modelo1$uniquenesses,decreasing = T)->u1
sort(modelo2$uniquenesses,decreasing = T)->u2
head(cbind(u1,u2))
scree(poly_cor)
fa.parallel(poly_cor,n.obs=200,fa="fa",fm="minres")
modelo_varimax<-fa(poly_cor,nfactors = 3,rotate = "varimax",
fa="mle")
print(modelo_varimax$loadings,cut=0)
fa.diagram(modelo_varimax)
library(stats)
factanal(datos_pca,factors=3, rotation="none")
library(MVN)
outliers <- mvn(data = datos_pca, mvnTest = "hz", multivariateOutlierMethod = "quan")
royston_test <- mvn(data = datos_pca, mvnTest = "royston", multivariatePlot = "qq")
royston_test$multivariateNormality
hz_test <- mvn(data = datos_pca, mvnTest = "hz")
hz_test$multivariateNormality
library(biotools)
#datos_enteros<-read.spss("DB_3.sav", to.data.frame = TRUE)
datos_enteros[15:34,1]<-"china"
datos_enteros[1:15,1]<-"aa"
datos_enteros$PAIS <- as.factor(datos_enteros$PAIS)
col_names <- names(datos_enteros[,2:12])
# to do it for some names in a vector named 'col_names'
datos_enteros[col_names] <- lapply(datos_enteros[col_names] , as.double)
datos_enteros[,1]
boxM(data = datos_enteros[, 2:12], grouping = datos_enteros[, 1])
library(MASS)
continente <- datos_enteros$continente
continente <- as.factor(continente)
modelo_lda <- lda(formula = continente ~ ZPOBDENS + ZTMINFAN, data = datos_enteros[,-1])
modelo_lda
nuevas_observaciones <- data.frame(ZPOBDENS = -0.5, ZTMINFAN = 0.7)
predict(object = modelo_lda, newdata = nuevas_observaciones)
library(biotools)
pred <- predict(modelo_lda, dimen = 1)
confusionmatrix(datos_enteros$continente, pred$class)
# Porcentaje de errores de clasificación
trainig_error <- mean(datos_enteros$continente != pred$class) * 100
paste("trainig_error=", trainig_error, "%")
library(klaR)
continente <- datos_enteros$continente
continente <- as.factor(continente)
partimat(continente ~ ZPOBDENS + ZTMINFAN,
data = datos_pca, method = "lda", prec = 200,
col.mean = "firebrick",nplots.vert =1, nplots.hor=3)
library(MASS)
#modelo_qda <- qda(formula = continente ~ ZPOBDENS + ZTMINFAN, data = datos_enteros[,-1])
#modelo_qda
library(tidyverse)
library(cluster)
library(factoextra)
# Para evitar que el análisis cluster se vea influido por cualquier variable arbitraria se estandarizan los datos
datos_pca<-scale(datos_pca)
head(datos_pca)
k4 <- kmeans(datos_pca, centers = 4, nstart = 25)
str(k4)
k4
fviz_cluster(k4,data=datos_pca)
set.seed(123)
fviz_nbclust(datos_pca, kmeans, method = "silhouette")
k2 <- kmeans(datos_pca, centers = 2, nstart = 123)
str(k2)
k2
fviz_cluster(k2,data=datos_pca)
